{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOmKsoyQmurV5nHdyniqbnJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unverciftci/RL_LLM/blob/main/RL_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Mbn2P3Z9BT1"
      },
      "outputs": [],
      "source": [
        "# LLM Reinforcement Learning Agent Demo\n",
        "# Run this in Google Colab - it uses Qwen3-0.6B to learn maze solving through trial and error\n",
        "\n",
        "# !pip install transformers torch accelerate\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output, display\n",
        "from collections import deque\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Initialize the model (Qwen3-0.6B - small enough for Colab)\n",
        "print(\"Loading Qwen3-0.6B model...\")\n",
        "model_name = \"Qwen/Qwen3-0.6B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Simple Maze Environment\n",
        "class MazeEnv:\n",
        "    def __init__(self, size=5):\n",
        "        self.size = size\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.player_pos = [0, 0]\n",
        "        self.goal_pos = [self.size-1, self.size-1]\n",
        "\n",
        "        # Create walls (1 = wall, 0 = empty)\n",
        "        self.maze = np.zeros((self.size, self.size))\n",
        "        # Add some walls to make it interesting\n",
        "        self.maze[1, 1:4] = 1\n",
        "        self.maze[2, 2] = 1\n",
        "        self.maze[3, 0:3] = 1\n",
        "\n",
        "        self.steps = 0\n",
        "        return self.get_state_description()\n",
        "\n",
        "    def get_state_description(self):\n",
        "        x, y = self.player_pos\n",
        "\n",
        "        # Check what's in each direction\n",
        "        directions = {\n",
        "            'North': [x-1, y] if x > 0 else None,\n",
        "            'South': [x+1, y] if x < self.size-1 else None,\n",
        "            'East': [x, y+1] if y < self.size-1 else None,\n",
        "            'West': [x, y-1] if y > 0 else None\n",
        "        }\n",
        "\n",
        "        walls = []\n",
        "        for dir_name, pos in directions.items():\n",
        "            if pos is None or self.maze[pos[0], pos[1]] == 1:\n",
        "                walls.append(dir_name)\n",
        "\n",
        "        # Calculate relative position to goal\n",
        "        goal_dir = []\n",
        "        if self.goal_pos[0] < x: goal_dir.append(\"North\")\n",
        "        if self.goal_pos[0] > x: goal_dir.append(\"South\")\n",
        "        if self.goal_pos[1] < y: goal_dir.append(\"West\")\n",
        "        if self.goal_pos[1] > y: goal_dir.append(\"East\")\n",
        "\n",
        "        state = f\"Position: ({x},{y}). \"\n",
        "        if walls:\n",
        "            state += f\"Walls at: {', '.join(walls)}. \"\n",
        "        state += f\"Goal is \"\n",
        "        if goal_dir:\n",
        "            state += f\"{' and '.join(goal_dir)}\"\n",
        "        else:\n",
        "            state += \"HERE!\"\n",
        "\n",
        "        return state\n",
        "\n",
        "    def step(self, action):\n",
        "        old_pos = self.player_pos.copy()\n",
        "\n",
        "        # Move based on action\n",
        "        if action == 'North' and self.player_pos[0] > 0:\n",
        "            self.player_pos[0] -= 1\n",
        "        elif action == 'South' and self.player_pos[0] < self.size-1:\n",
        "            self.player_pos[0] += 1\n",
        "        elif action == 'East' and self.player_pos[1] < self.size-1:\n",
        "            self.player_pos[1] += 1\n",
        "        elif action == 'West' and self.player_pos[1] > 0:\n",
        "            self.player_pos[1] -= 1\n",
        "\n",
        "        # Check if hit wall\n",
        "        if self.maze[self.player_pos[0], self.player_pos[1]] == 1:\n",
        "            self.player_pos = old_pos  # Bounce back\n",
        "            reward = -0.5  # Penalty for hitting wall\n",
        "        elif self.player_pos == self.goal_pos:\n",
        "            reward = 10  # Big reward for reaching goal\n",
        "        else:\n",
        "            reward = -0.1  # Small penalty for each step\n",
        "\n",
        "        self.steps += 1\n",
        "        done = (self.player_pos == self.goal_pos) or (self.steps > 50)\n",
        "\n",
        "        return self.get_state_description(), reward, done\n",
        "\n",
        "    def render(self):\n",
        "        maze_display = np.zeros((self.size, self.size))\n",
        "        maze_display[self.maze == 1] = 0.5  # Walls\n",
        "        maze_display[self.player_pos[0], self.player_pos[1]] = 1  # Player\n",
        "        maze_display[self.goal_pos[0], self.goal_pos[1]] = 0.8  # Goal\n",
        "\n",
        "        plt.imshow(maze_display, cmap='coolwarm')\n",
        "        plt.title(f\"Step: {self.steps}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "# LLM Agent with Memory\n",
        "class LLMAgent:\n",
        "    def __init__(self, model, tokenizer, temperature=0.7):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.memory = deque(maxlen=20)  # Remember last 20 experiences\n",
        "        self.temperature = temperature\n",
        "        self.episode_count = 0\n",
        "\n",
        "    def build_prompt(self, state):\n",
        "        # Build a prompt that includes recent experiences\n",
        "        prompt = \"You are learning to navigate a maze through trial and error.\\n\\n\"\n",
        "\n",
        "        if len(self.memory) > 0:\n",
        "            prompt += \"Your recent experiences:\\n\"\n",
        "            for exp in list(self.memory)[-10:]:  # Use last 10 experiences\n",
        "                prompt += f\"- At {exp['state'][:20]}..., took action {exp['action']} â†’ \"\n",
        "                if exp['reward'] > 5:\n",
        "                    prompt += f\"REACHED GOAL! (reward: {exp['reward']:.1f})\\n\"\n",
        "                elif exp['reward'] < -0.3:\n",
        "                    prompt += f\"hit wall (reward: {exp['reward']:.1f})\\n\"\n",
        "                else:\n",
        "                    prompt += f\"moved forward (reward: {exp['reward']:.1f})\\n\"\n",
        "            prompt += \"\\n\"\n",
        "\n",
        "        prompt += f\"Current state: {state}\\n\"\n",
        "        prompt += \"Based on your experience, choose the best action.\\n\"\n",
        "        prompt += \"Available actions: North, South, East, West\\n\"\n",
        "        prompt += \"Your action:\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def get_action(self, state, epsilon=0.1):\n",
        "        # Epsilon-greedy exploration\n",
        "        if np.random.random() < epsilon:\n",
        "            return np.random.choice(['North', 'South', 'East', 'West'])\n",
        "\n",
        "        prompt = self.build_prompt(state)\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=10,\n",
        "                temperature=self.temperature,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "        # Parse action from response\n",
        "        action = None\n",
        "        for act in ['North', 'South', 'East', 'West']:\n",
        "            if act.lower() in response.lower():\n",
        "                action = act\n",
        "                break\n",
        "\n",
        "        if action is None:\n",
        "            action = np.random.choice(['North', 'South', 'East', 'West'])\n",
        "\n",
        "        return action\n",
        "\n",
        "    def update_memory(self, state, action, reward, next_state):\n",
        "        self.memory.append({\n",
        "            'state': state,\n",
        "            'action': action,\n",
        "            'reward': reward,\n",
        "            'next_state': next_state\n",
        "        })\n",
        "\n",
        "# Training Loop\n",
        "def train_agent(episodes=20):\n",
        "    env = MazeEnv(size=5)\n",
        "    agent = LLMAgent(model, tokenizer)\n",
        "\n",
        "    episode_rewards = []\n",
        "    episode_steps = []\n",
        "    success_rate = deque(maxlen=10)\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        # Decay epsilon over time\n",
        "        epsilon = max(0.1, 0.5 - episode * 0.02)\n",
        "\n",
        "        print(f\"\\n=== Episode {episode + 1} ===\")\n",
        "\n",
        "        while not done:\n",
        "            # Get action from LLM\n",
        "            action = agent.get_action(state, epsilon)\n",
        "\n",
        "            # Take action in environment\n",
        "            next_state, reward, done = env.step(action)\n",
        "\n",
        "            # Update agent's memory\n",
        "            agent.update_memory(state, action, reward, next_state)\n",
        "\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            # Print step info\n",
        "            if reward > 5:\n",
        "                print(f\"ðŸŽ¯ GOAL REACHED! Action: {action}\")\n",
        "            elif reward < -0.3:\n",
        "                print(f\"ðŸ’¥ Hit wall with action: {action}\")\n",
        "            else:\n",
        "                print(f\"â†’ Moved {action}, new position in state\")\n",
        "\n",
        "        episode_rewards.append(total_reward)\n",
        "        episode_steps.append(env.steps)\n",
        "        success_rate.append(1 if env.player_pos == env.goal_pos else 0)\n",
        "\n",
        "        print(f\"Episode reward: {total_reward:.2f}, Steps: {env.steps}\")\n",
        "        print(f\"Success rate (last 10): {np.mean(success_rate)*100:.1f}%\")\n",
        "\n",
        "        # Visualize the final position\n",
        "        if episode % 5 == 0 or episode == episodes - 1:\n",
        "            env.render()\n",
        "\n",
        "    # Plot learning curves\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    ax1.plot(episode_rewards)\n",
        "    ax1.set_xlabel('Episode')\n",
        "    ax1.set_ylabel('Total Reward')\n",
        "    ax1.set_title('Learning Progress: Rewards')\n",
        "    ax1.grid(True)\n",
        "\n",
        "    ax2.plot(episode_steps)\n",
        "    ax2.set_xlabel('Episode')\n",
        "    ax2.set_ylabel('Steps to Complete')\n",
        "    ax2.set_title('Learning Progress: Efficiency')\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return agent\n",
        "\n",
        "# Run the training\n",
        "print(\"\\nðŸš€ Starting LLM RL Training...\\n\")\n",
        "print(\"The agent will learn to navigate a maze using only trial and error!\")\n",
        "print(\"Watch as it builds memory of what works and what doesn't.\\n\")\n",
        "\n",
        "trained_agent = train_agent(episodes=20)\n",
        "\n",
        "print(\"\\nâœ… Training Complete!\")\n",
        "print(\"\\nThe LLM has learned to solve the maze through reinforcement learning,\")\n",
        "print(\"using its memory of past experiences to make better decisions.\")\n",
        "\n",
        "# Test the trained agent\n",
        "print(\"\\nðŸ§ª Testing trained agent (without exploration)...\")\n",
        "env = MazeEnv(size=5)\n",
        "state = env.reset()\n",
        "done = False\n",
        "test_reward = 0\n",
        "\n",
        "print(\"\\nFinal test run:\")\n",
        "while not done and env.steps < 30:\n",
        "    action = trained_agent.get_action(state, epsilon=0)  # No exploration\n",
        "    state, reward, done = env.step(action)\n",
        "    test_reward += reward\n",
        "    print(f\"Step {env.steps}: {action} â†’ Reward: {reward:.2f}\")\n",
        "\n",
        "env.render()\n",
        "print(f\"\\nTest episode reward: {test_reward:.2f}\")\n",
        "if env.player_pos == env.goal_pos:\n",
        "    print(\"ðŸŽ‰ Successfully reached the goal!\")\n",
        "else:\n",
        "    print(\"â±ï¸ Timed out\")"
      ]
    }
  ]
}